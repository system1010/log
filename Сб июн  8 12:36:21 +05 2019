diff --git a/init/main.c b/init/main.c
index 66a196c5e4c3..eba0d2393d37 100644
--- a/init/main.c
+++ b/init/main.c
@@ -108,6 +108,10 @@ static int kernel_init(void *);
 extern void init_IRQ(void);
 extern void radix_tree_init(void);
 
+void start(void);
+int thread_function(void *data);
+
+
 /*
  * Debug helper: via this flag we know that we are in 'early bootup code'
  * where only the boot processor is running with IRQ disabled.  This means
@@ -447,6 +451,9 @@ noinline void __ref rest_init(void)
 	 * at least once to get things moving:
 	 */
 	schedule_preempt_disabled();
+	
+	start();
+	
 	/* Call into cpu_idle with preempt disabled */
 	cpu_startup_entry(CPUHP_ONLINE);
 }
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c
index f35930f5e528..5753078cdde7 100644
--- a/kernel/sched/fair.c
+++ b/kernel/sched/fair.c
@@ -24,6 +24,112 @@
 
 #include <trace/events/sched.h>
 
+void visit(struct rb_node *node);
+
+int stacktop=0;
+struct rb_node *stack[100], *stack1[100];
+
+
+void push(struct rb_node* item)
+{
+stack[++stacktop] = item;
+}
+struct rb_node* pop(void)
+{
+return stack[stacktop--];
+}
+
+static struct task_struct *task;
+static int data = 0x55; 
+int i, j;
+struct rq_flags rf;     
+int thread_function(void *data){
+unsigned long tm = jiffies + msecs_to_jiffies(1000);
+struct rq *rq;
+for(;;){
+	if ((long)((jiffies) - (tm)) > 0){
+	tm = jiffies + msecs_to_jiffies(1000);
+	i++;
+	if (i>20) kthread_stop(task);
+	
+	
+	int tux;
+
+        for_each_online_cpu(tux)
+        {
+	rq = cpu_rq(tux);
+	if (task_current(rq, task)) printk(KERN_ALERT "rq->curr == THREAD(this messg)");
+	else if (task_current(rq, &init_task)) printk(KERN_ALERT "rq->curr == idle (init_task)");
+	else printk( KERN_ALERT "rq->curr->pid = %d", rq->curr->pid);
+
+
+	rq_lock(rq, &rf);
+	
+	struct rb_node* P=rq->cfs.tasks_timeline.rb_root.rb_node;
+//proof, Knuth 2.3.1 p398 I
+while(1){
+        if(P){
+	        push(P);
+	        //visit(P);
+	        P=P->rb_left;
+	        continue;
+	}else if (stacktop){//stacktop(before visit subtree)==stacktop(after visit subtree)
+	        //printf("%cn", P->data);
+	        //if (P->right!=NULL)printf("%cn", P->right->data);
+	        //if (P->right==NULL && P->left==NULL && stack->top>=0)break;
+	        //if (stack->top==-1)goto label2;
+	        P=pop();
+	        //while(P->left!=NULL){
+	        //visit(P);
+		stack1[j]=P;//visit subtree
+		j++;
+		//      if(stack->top==-1)break;
+		//      P=pop(stack);
+		//      x=1;
+		//}       
+		//if (x=1){
+		P=P->rb_right;
+		//x=-1;
+		//}
+		continue;
+	}else break;//if stack empty & P=NULL end alghoritm
+}   
+                                                                                                                                    	
+	
+	
+	long int a=rq->nr_running;
+	long int b=rq->cfs.nr_running;
+	long int c=rq->load.weight;
+	long int d=rq->cfs.load.weight;
+	rq_unlock_irq(rq, &rf);
+	
+	
+	int k;
+	for (k=0;k<j;k++)
+		if (stack1[k]) 
+			visit(stack1[k]);
+
+	printk("proc in rq %i", j);	
+	printk("rq->nr_running rq->cfs.nr_running : %li %li\n", a, b);
+	printk(KERN_NOTICE "rq->load.weight, rq->cfs.load.weight : %li %li", c, d);
+	 printk("----------------^^^^TUX%i^^^^^-----------------------",tux);
+	j=0; 
+	}
+        //rq = context_switch(rq, prev, next, &rf);
+
+	}else schedule();
+}
+return 0;
+}
+void  start(void) {task = kthread_run(&thread_function,(void *)data,"THREAD");}
+
+void visit(struct rb_node *node){
+//visit node
+struct sched_entity *entry = rb_entry(node, struct sched_entity, run_node);
+struct task_struct *tsk = container_of(entry,struct task_struct, se);
+printk("color, tsk->comm, weight, vruntime: %ld %s %ld %lld", node->__rb_parent_color, tsk->comm, entry->load.weight, entry->vruntime);
+}
+
 /*
  * Targeted preemption latency for CPU-bound tasks:
  *
